{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e783bbc5-ff99-49c8-8fa5-5b682cf5056f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1a50275-7b0d-4f9c-8e9d-4aaa6fa2d12d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audio_table = spark.table(\"marcell.call_centre_processing.recording_file_reference\")\\\n",
    "  .withColumn(\"modification_timestamp\", F.from_unixtime(F.col(\"modificationTime\")/1000).cast(\"timestamp\"))\\\n",
    "  .withColumn(\"recording_timestamp\", F.expr(\"timestamp_seconds(cast(rand() * (unix_timestamp('2025-01-27 23:59:59') - unix_timestamp('2025-01-01 00:00:00')) as int) + unix_timestamp('2025-01-01 00:00:00'))\"))\n",
    "\n",
    "num_partitions = 4 # enforce to number of GPU workers?\n",
    "audio_table = audio_table.repartition(num_partitions).cache()\n",
    "audio_table.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "546f6e26-ed3e-40dd-90ac-990b00dce4d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d9481c6-e516-4af8-a107-593f91ab024e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@F.pandas_udf(\"string\")\n",
    "def transcribe(path_iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    import torch\n",
    "    from transformers import pipeline\n",
    "    from transformers.utils import is_flash_attn_2_available\n",
    "    import json\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=\"openai/whisper-medium\", # select checkpoint from https://huggingface.co/openai/whisper-large-v3#model-details\n",
    "        # model=\"distil-whisper/large-v2\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device=\"cuda:0\", # or mps for Mac devices\n",
    "        # Comment this out if you plan on using T4 gpus, flash attention is only supported on ampere (A10/A100) architecture\n",
    "        model_kwargs={\"attn_implementation\": \"flash_attention_2\"} if is_flash_attn_2_available() else {\"attn_implementation\": \"sdpa\"},\n",
    "    )\n",
    "\n",
    "    def add_suffix(path: str):\n",
    "        outputs = pipe(\n",
    "            path,\n",
    "            chunk_length_s=30,\n",
    "            batch_size=24,\n",
    "            return_timestamps=True,\n",
    "        )\n",
    "        return json.dumps(outputs[\"chunks\"])\n",
    "\n",
    "    for path_chunk in path_iterator:\n",
    "        yield path_chunk.apply(add_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df31c6d6-fc81-4840-811f-6878d7805c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audio_table.withColumn(\"transcription\", transcribe(\"file_path\")).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"marcell.call_centre_processing.transcriptions_udf\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01b Transcribe with Transformers and Pandas UDF",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
