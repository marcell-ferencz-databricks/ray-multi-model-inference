{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919e189e-a63d-4cfb-bddb-2edc95428b5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"marcell\"\n",
    "SCHEMA = \"call_centre_processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "303da576-73bb-453a-9c33-9975142deb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.my_volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6fafd7b-3d7e-428b-8068-f3f96ae34b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 00:24:16.174855: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03525a51-e498-484a-b9b4-8e8a818534e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d12ce6caf14ef794b4e9b4d3841343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18aac9204d6549cf92fa6a1404fb44bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73943d95703a42469b26d260204ca53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2feae777f9b248e5be89e7e712ecebd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee40e2b2434540a98014bcb9160be46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442f8539f4eb4f198be8984c453aa0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f209e00bee49178b018313395f6113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec4c2af22e74faebdd324d54570b827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b4a8c256f94e1995c3687649d01f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80a9f29d30f498ebe281e13b4e2fac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df19194c36fc45c19ccd02257b2b998d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_SAVE_PATH = \"/Volumes/marcell/call_centre_processing/data/model_artifacts/whisper-tiny/\"\n",
    "dbutils.fs.mkdirs(MODEL_SAVE_PATH)\n",
    "MODEL_ID = \"openai/whisper-tiny\"\n",
    "whisper_pipeline = pipeline(\"automatic-speech-recognition\", MODEL_ID, torch_dtype=torch.float16, device=\"cuda:0\")\n",
    "whisper_pipeline.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f0d09b-0601-4463-a554-ff30b20d66a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb912c6a0b44a4f993122bfee5a2409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e7cdfce04b4865b3ab0640dc9d0708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6b8d08dd9041179a7ef552d17ec2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4aaf69c54f84682b3f3bfd5010405da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d1fd42238446de830a16959629bd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ed28fbf07746048abd168343e91cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176a24a804ec4186aadbbe5b59a67125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6c50b56c484ce19c5000af0a9e4037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eff903e09ef4b36a08726682928e7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4d0785fe0c4095b124669c1e8843a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ee256037834fee9603ac8c94d53a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MODEL_SAVE_PATH = \"/Volumes/marcell/call_centre_processing/data/model_artifacts/whisper-tiny/\"\n",
    "# dbutils.fs.mkdirs(MODEL_SAVE_PATH)\n",
    "MODEL_ID = \"openai/whisper-tiny\"\n",
    "whisper_pipeline = pipeline(\"automatic-speech-recognition\", MODEL_ID, torch_dtype=torch.float16, device=\"cuda:0\")\n",
    "# whisper_pipeline.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6843f41a-e77c-4715-adf8-ffd2dfcc0c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.transformers.log_model(whisper_pipeline,artifact_path='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73182c2b-6f35-488e-b0fb-21aadaa31903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51649c67b49c4ceaa73cb3b16e75b0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66419dbc12b4b308a5b7275feefce54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9c6723297f405ca146c8f0761934f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e53823f035464e859884a24edeb5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd28540aaa0480194422d86ddbd85a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943611c6425c4708b5c43f0cb95cdb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e572eaa852042108f36d818f03973d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305f3b09264c46adb6c4b2d89c29f955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fef5c7e19f44c86a053759ee635d25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d1960ee3e544b09d57b5c504db45ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce6227b9a3143709788136e525b3b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_SAVE_PATH = \"/Volumes/marcell/call_centre_processing/data/model_artifacts/whisper-medium/\"\n",
    "dbutils.fs.mkdirs(MODEL_SAVE_PATH)\n",
    "MODEL_ID = \"openai/whisper-large-v3\"\n",
    "whisper_pipeline = pipeline(\"automatic-speech-recognition\", MODEL_ID, torch_dtype=torch.float16, device=\"cuda:0\")\n",
    "# whisper_pipeline.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb4ed821-8158-4254-9c8a-84d82cdec8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-11 00:29:22,626] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n/usr/bin/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93m [WARNING] \u001B[0m async_io requires the dev libaio .so object and headers but these were not found.\n\u001B[93m [WARNING] \u001B[0m async_io: please install the libaio-dev package with apt\n\u001B[93m [WARNING] \u001B[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\u001B[93m [WARNING] \u001B[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\u001B[93m [WARNING] \u001B[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n\u001B[93m [WARNING] \u001B[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dd11acc04f4d0b83ce48c2bd7db016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5824e6072f5f417eac0de3edce49faac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1623ca8e17b54dc4a33564b71be2898c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /local_disk0/repl_tmp_data/ReplId-194f2-27f91-f/tmp69x6tbia/model/model/model-00001-of-00007.safetenâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: dbstorage65kk6cs2ttmou.blob.core.windows.net. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: dbstorage65kk6cs2ttmou.blob.core.windows.net. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: dbstorage65kk6cs2ttmou.blob.core.windows.net. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: dbstorage65kk6cs2ttmou.blob.core.windows.net. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: dbstorage65kk6cs2ttmou.blob.core.windows.net. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: dbstorage65kk6cs2ttmou.blob.core.windows.net. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: dbstorage65kk6cs2ttmou.blob.core.windows.net. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: dbstorage65kk6cs2ttmou.blob.core.windows.net. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: dbstorage65kk6cs2ttmou.blob.core.windows.net. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: dbstorage65kk6cs2ttmou.blob.core.windows.net. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: dbstorage65kk6cs2ttmou.blob.core.windows.net. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: dbstorage65kk6cs2ttmou.blob.core.windows.net. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: dbstorage65kk6cs2ttmou.blob.core.windows.net. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x7f0ceae782d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.transformers.log_model(whisper_pipeline,\n",
    "        artifact_path='model',\n",
    "        registered_model_name=\"mlops_pj.gsk_gsc_cfu_count.whisper_large_v3\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0d26a3a-c57e-49ab-b45d-acbd26e5e69e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"/Volumes/marcell/call_centre_processing/data/model_artifacts/ner-model/\"\n",
    "dbutils.fs.mkdirs(MODEL_SAVE_PATH)\n",
    "ner_pipeline = pipeline(\"ner\", device=\"cuda:0\")\n",
    "ner_pipeline.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c4b67d-2d0e-4336-a5f3-ce9dc2513e95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c182ff8cb940678ae6276578bd2841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/820 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8475b7ae3f9646f0b3cf7bf8385e3557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7aa9d7139343e4a0ac09482bc084ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75138df719e5471bbd529bfb3048bec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75960f686e8a46daa9fa6311e682435d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c849533a064bf2b33de166ff9bbe08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a00cdd7ac74bc0995646f94ae106b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49860359285d472c8940e530094f5d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d606910c0949709c2b308396d3292f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b50be7040e4972b1939674a22007e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626db47714214eae8d7c5ccc8caf2efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78743fa213e24114b40f5a5bd42ddb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5428567da42641afaca3de7429b8f646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.61M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db213aa7897d439fb96aba9986d13b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/917k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404e00b85c0f4ef69d823fd24bf26667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdd1009112a4d81890481fb4b921914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a7b4823c6346afab6909f77bb9efef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_SAVE_PATH = \"/Volumes/marcell/call_centre_processing/data/model_artifacts/phi-4/\"\n",
    "dbutils.fs.mkdirs(MODEL_SAVE_PATH)\n",
    "phi_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"microsoft/phi-4\",\n",
    "        model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "# phi_pipeline.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0224a23e-b4f4-4afe-b314-b3fe48391668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1242762840119672>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmlflow\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mstart_run():\n",
       "\u001B[1;32m      4\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mtransformers\u001B[38;5;241m.\u001B[39mlog_model(phi_pipeline,\n",
       "\u001B[1;32m      5\u001B[0m         artifact_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m,\n",
       "\u001B[1;32m      6\u001B[0m         registered_model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmlops_pj.gsk_gsc_cfu_count.phi-4\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      7\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/mlflow/tracking/fluent.py:319\u001B[0m, in \u001B[0;36mstart_run\u001B[0;34m(run_id, experiment_id, run_name, nested, parent_run_id, tags, description, log_system_metrics)\u001B[0m\n",
       "\u001B[1;32m    317\u001B[0m experiment_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(experiment_id) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(experiment_id, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m experiment_id\n",
       "\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(_active_run_stack) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m nested:\n",
       "\u001B[0;32m--> 319\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n",
       "\u001B[1;32m    320\u001B[0m         (\n",
       "\u001B[1;32m    321\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRun with UUID \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m is already active. To start a new run, first end the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    322\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcurrent run with mlflow.end_run(). To start a nested \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    323\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun, call start_run with nested=True\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    324\u001B[0m         )\u001B[38;5;241m.\u001B[39mformat(_active_run_stack[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mrun_id)\n",
       "\u001B[1;32m    325\u001B[0m     )\n",
       "\u001B[1;32m    326\u001B[0m client \u001B[38;5;241m=\u001B[39m MlflowClient()\n",
       "\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_id:\n",
       "\n",
       "\u001B[0;31mException\u001B[0m: Run with UUID c6b727fdb69648b49c2ce36ae2054527 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Exception",
        "evalue": "Run with UUID c6b727fdb69648b49c2ce36ae2054527 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Exception</span>: Run with UUID c6b727fdb69648b49c2ce36ae2054527 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-1242762840119672>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmlflow\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mstart_run():\n\u001B[1;32m      4\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mtransformers\u001B[38;5;241m.\u001B[39mlog_model(phi_pipeline,\n\u001B[1;32m      5\u001B[0m         artifact_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      6\u001B[0m         registered_model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmlops_pj.gsk_gsc_cfu_count.phi-4\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      7\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/mlflow/tracking/fluent.py:319\u001B[0m, in \u001B[0;36mstart_run\u001B[0;34m(run_id, experiment_id, run_name, nested, parent_run_id, tags, description, log_system_metrics)\u001B[0m\n\u001B[1;32m    317\u001B[0m experiment_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(experiment_id) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(experiment_id, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m experiment_id\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(_active_run_stack) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m nested:\n\u001B[0;32m--> 319\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m    320\u001B[0m         (\n\u001B[1;32m    321\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRun with UUID \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m is already active. To start a new run, first end the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    322\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcurrent run with mlflow.end_run(). To start a nested \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    323\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun, call start_run with nested=True\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    324\u001B[0m         )\u001B[38;5;241m.\u001B[39mformat(_active_run_stack[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mrun_id)\n\u001B[1;32m    325\u001B[0m     )\n\u001B[1;32m    326\u001B[0m client \u001B[38;5;241m=\u001B[39m MlflowClient()\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_id:\n",
        "\u001B[0;31mException\u001B[0m: Run with UUID c6b727fdb69648b49c2ce36ae2054527 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.transformers.log_model(phi_pipeline,\n",
    "        artifact_path='model',\n",
    "        registered_model_name=\"mlops_pj.gsk_gsc_cfu_count.phi-4\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592e6f12-f30c-4235-8ff3-8ae470a0ea79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4389956761228159>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m MODEL_SAVE_PATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Volumes/marcell/call_centre_processing/data/model_artifacts/phi-3.5/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      2\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmkdirs(MODEL_SAVE_PATH)\n",
       "\u001B[0;32m----> 3\u001B[0m phi_35_pipeline \u001B[38;5;241m=\u001B[39m pipeline(\n",
       "\u001B[1;32m      4\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      5\u001B[0m         model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmicrosoft/Phi-3.5-mini-instruct\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      6\u001B[0m     )\n",
       "\u001B[1;32m      7\u001B[0m phi_35_pipeline\u001B[38;5;241m.\u001B[39msave_pretrained(MODEL_SAVE_PATH)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/transformers/pipelines/__init__.py:816\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    813\u001B[0m                 adapter_config \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n",
       "\u001B[1;32m    814\u001B[0m                 model \u001B[38;5;241m=\u001B[39m adapter_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbase_model_name_or_path\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[0;32m--> 816\u001B[0m     config \u001B[38;5;241m=\u001B[39m AutoConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n",
       "\u001B[1;32m    817\u001B[0m         model, _from_pipeline\u001B[38;5;241m=\u001B[39mtask, code_revision\u001B[38;5;241m=\u001B[39mcode_revision, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs\n",
       "\u001B[1;32m    818\u001B[0m     )\n",
       "\u001B[1;32m    819\u001B[0m     hub_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39m_commit_hash\n",
       "\u001B[1;32m    821\u001B[0m custom_tasks \u001B[38;5;241m=\u001B[39m {}\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:958\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    952\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n",
       "\u001B[1;32m    953\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    954\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe checkpoint you are trying to load has model type `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    955\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    956\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    957\u001B[0m         )\n",
       "\u001B[0;32m--> 958\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m config_class\u001B[38;5;241m.\u001B[39mfrom_dict(config_dict, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munused_kwargs)\n",
       "\u001B[1;32m    959\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    960\u001B[0m     \u001B[38;5;66;03m# Fallback: use pattern matching on the string.\u001B[39;00m\n",
       "\u001B[1;32m    961\u001B[0m     \u001B[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001B[39;00m\n",
       "\u001B[1;32m    962\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m pattern \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(CONFIG_MAPPING\u001B[38;5;241m.\u001B[39mkeys(), key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m, reverse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/transformers/configuration_utils.py:768\u001B[0m, in \u001B[0;36mPretrainedConfig.from_dict\u001B[0;34m(cls, config_dict, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    765\u001B[0m \u001B[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001B[39;00m\n",
       "\u001B[1;32m    766\u001B[0m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattn_implementation\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattn_implementation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\u001B[0;32m--> 768\u001B[0m config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_dict)\n",
       "\u001B[1;32m    770\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpruned_heads\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[1;32m    771\u001B[0m     config\u001B[38;5;241m.\u001B[39mpruned_heads \u001B[38;5;241m=\u001B[39m {\u001B[38;5;28mint\u001B[39m(key): value \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m config\u001B[38;5;241m.\u001B[39mpruned_heads\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/transformers/models/phi3/configuration_phi3.py:159\u001B[0m, in \u001B[0;36mPhi3Config.__init__\u001B[0;34m(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, resid_pdrop, embd_pdrop, attention_dropout, hidden_act, max_position_embeddings, original_max_position_embeddings, initializer_range, rms_norm_eps, use_cache, tie_word_embeddings, rope_theta, rope_scaling, bos_token_id, eos_token_id, pad_token_id, sliding_window, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    157\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrope_theta \u001B[38;5;241m=\u001B[39m rope_theta\n",
       "\u001B[1;32m    158\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrope_scaling \u001B[38;5;241m=\u001B[39m rope_scaling\n",
       "\u001B[0;32m--> 159\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rope_scaling_validation()\n",
       "\u001B[1;32m    160\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msliding_window \u001B[38;5;241m=\u001B[39m sliding_window\n",
       "\u001B[1;32m    162\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n",
       "\u001B[1;32m    163\u001B[0m     bos_token_id\u001B[38;5;241m=\u001B[39mbos_token_id,\n",
       "\u001B[1;32m    164\u001B[0m     eos_token_id\u001B[38;5;241m=\u001B[39meos_token_id,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    167\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n",
       "\u001B[1;32m    168\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/transformers/models/phi3/configuration_phi3.py:186\u001B[0m, in \u001B[0;36mPhi3Config._rope_scaling_validation\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    184\u001B[0m rope_scaling_long_factor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrope_scaling\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlong_factor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rope_scaling_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m rope_scaling_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myarn\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
       "\u001B[0;32m--> 186\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`rope_scaling`\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms type field must be one of [\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124myarn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m], got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrope_scaling_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\n",
       "\u001B[1;32m    188\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(rope_scaling_short_factor, \u001B[38;5;28mlist\u001B[39m)\n",
       "\u001B[1;32m    189\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, (\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m)) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m rope_scaling_short_factor)\n",
       "\u001B[1;32m    190\u001B[0m ):\n",
       "\u001B[1;32m    191\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    192\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`rope_scaling`\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms short_factor field must be a list of numbers, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrope_scaling_short_factor\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    193\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: `rope_scaling`'s type field must be one of ['su', 'yarn'], got longrope"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ValueError",
        "evalue": "`rope_scaling`'s type field must be one of ['su', 'yarn'], got longrope"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: `rope_scaling`'s type field must be one of ['su', 'yarn'], got longrope"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
        "File \u001B[0;32m<command-4389956761228159>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m MODEL_SAVE_PATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Volumes/marcell/call_centre_processing/data/model_artifacts/phi-3.5/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      2\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmkdirs(MODEL_SAVE_PATH)\n\u001B[0;32m----> 3\u001B[0m phi_35_pipeline \u001B[38;5;241m=\u001B[39m pipeline(\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m         model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmicrosoft/Phi-3.5-mini-instruct\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      6\u001B[0m     )\n\u001B[1;32m      7\u001B[0m phi_35_pipeline\u001B[38;5;241m.\u001B[39msave_pretrained(MODEL_SAVE_PATH)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/transformers/pipelines/__init__.py:816\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[1;32m    813\u001B[0m                 adapter_config \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m    814\u001B[0m                 model \u001B[38;5;241m=\u001B[39m adapter_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbase_model_name_or_path\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m--> 816\u001B[0m     config \u001B[38;5;241m=\u001B[39m AutoConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    817\u001B[0m         model, _from_pipeline\u001B[38;5;241m=\u001B[39mtask, code_revision\u001B[38;5;241m=\u001B[39mcode_revision, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs\n\u001B[1;32m    818\u001B[0m     )\n\u001B[1;32m    819\u001B[0m     hub_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39m_commit_hash\n\u001B[1;32m    821\u001B[0m custom_tasks \u001B[38;5;241m=\u001B[39m {}\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:958\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    952\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m    953\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    954\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe checkpoint you are trying to load has model type `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    955\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    956\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    957\u001B[0m         )\n\u001B[0;32m--> 958\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m config_class\u001B[38;5;241m.\u001B[39mfrom_dict(config_dict, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munused_kwargs)\n\u001B[1;32m    959\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    960\u001B[0m     \u001B[38;5;66;03m# Fallback: use pattern matching on the string.\u001B[39;00m\n\u001B[1;32m    961\u001B[0m     \u001B[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001B[39;00m\n\u001B[1;32m    962\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m pattern \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(CONFIG_MAPPING\u001B[38;5;241m.\u001B[39mkeys(), key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m, reverse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/transformers/configuration_utils.py:768\u001B[0m, in \u001B[0;36mPretrainedConfig.from_dict\u001B[0;34m(cls, config_dict, **kwargs)\u001B[0m\n\u001B[1;32m    765\u001B[0m \u001B[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001B[39;00m\n\u001B[1;32m    766\u001B[0m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattn_implementation\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattn_implementation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 768\u001B[0m config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_dict)\n\u001B[1;32m    770\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpruned_heads\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    771\u001B[0m     config\u001B[38;5;241m.\u001B[39mpruned_heads \u001B[38;5;241m=\u001B[39m {\u001B[38;5;28mint\u001B[39m(key): value \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m config\u001B[38;5;241m.\u001B[39mpruned_heads\u001B[38;5;241m.\u001B[39mitems()}\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/transformers/models/phi3/configuration_phi3.py:159\u001B[0m, in \u001B[0;36mPhi3Config.__init__\u001B[0;34m(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, resid_pdrop, embd_pdrop, attention_dropout, hidden_act, max_position_embeddings, original_max_position_embeddings, initializer_range, rms_norm_eps, use_cache, tie_word_embeddings, rope_theta, rope_scaling, bos_token_id, eos_token_id, pad_token_id, sliding_window, **kwargs)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrope_theta \u001B[38;5;241m=\u001B[39m rope_theta\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrope_scaling \u001B[38;5;241m=\u001B[39m rope_scaling\n\u001B[0;32m--> 159\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rope_scaling_validation()\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msliding_window \u001B[38;5;241m=\u001B[39m sliding_window\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    163\u001B[0m     bos_token_id\u001B[38;5;241m=\u001B[39mbos_token_id,\n\u001B[1;32m    164\u001B[0m     eos_token_id\u001B[38;5;241m=\u001B[39meos_token_id,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    168\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/transformers/models/phi3/configuration_phi3.py:186\u001B[0m, in \u001B[0;36mPhi3Config._rope_scaling_validation\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    184\u001B[0m rope_scaling_long_factor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrope_scaling\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlong_factor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rope_scaling_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m rope_scaling_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myarn\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 186\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`rope_scaling`\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms type field must be one of [\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124myarn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m], got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrope_scaling_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\n\u001B[1;32m    188\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(rope_scaling_short_factor, \u001B[38;5;28mlist\u001B[39m)\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, (\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m)) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m rope_scaling_short_factor)\n\u001B[1;32m    190\u001B[0m ):\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    192\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`rope_scaling`\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms short_factor field must be a list of numbers, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrope_scaling_short_factor\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    193\u001B[0m     )\n",
        "\u001B[0;31mValueError\u001B[0m: `rope_scaling`'s type field must be one of ['su', 'yarn'], got longrope"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_SAVE_PATH = \"/Volumes/marcell/call_centre_processing/data/model_artifacts/phi-3.5/\"\n",
    "dbutils.fs.mkdirs(MODEL_SAVE_PATH)\n",
    "phi_35_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"microsoft/Phi-3.5-mini-instruct\"\n",
    "    )\n",
    "phi_35_pipeline.save_pretrained(MODEL_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_download_huggingface_models",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}