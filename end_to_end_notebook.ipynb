{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-model inference with Ray on Databricks\n",
    "\n",
    "This notebook demonstrates how to perform multi-model inference using Ray on Databricks. We will:\n",
    "\n",
    "1. Set up the environment and install necessary packages.\n",
    "3. Download and prepare the LJSpeech dataset.\n",
    "5. Download and set up models from Hugging Face.\n",
    "6. Define classes to handle each inference step, including audio conversion, transcription, named entity redaction, and text classification.\n",
    "7. Run inference using Ray Data native commands and save the results to a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using >=0.7.0 as it supports whisper and manually updating numba due to conflicts\n",
    "%pip install vllm==0.7.0 pydub numba==0.61.0 databricks-sdk \n",
    "%pip install ray --upgrade\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set catalog and schema\n",
    "\n",
    "We set the catalog and schema to organise our data and ensure it is stored in the correct location. Change these to suit your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = \"marcell\"\n",
    "SCHEMA = \"call_centre_processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create catalog, schema and volume if they don't exist, and create directories for compressed, raw audio files and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.data\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{CATALOG}/{SCHEMA}/data/compressed/LJSpeech\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{CATALOG}/{SCHEMA}/data/raw_audio/LJSpeech\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{CATALOG}/{SCHEMA}/data/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download raw audio files\n",
    "\n",
    "We download the [LJSpeech dataset](https://paperswithcode.com/dataset/ljspeech) from the URL and unzip it to the raw audio directory. This is a collection of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. The files are stored in a tar.bz2 archive, so we will first download it and then unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the LJSpeech dataset\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
    "target_file_path = f\"/Volumes/{CATALOG}/{SCHEMA}/data/compressed/LJSpeech/LJSpeech-1.1.tar.bz2\"\n",
    "urllib.request.urlretrieve(url, target_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unzipping can take quite some time (>1hr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec1f1ea-1573-48e9-b667-f7b60838054f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unzip the LJSpeech dataset\n",
    "\n",
    "import tarfile\n",
    "\n",
    "extract_to_path = f\"/Volumes/{CATALOG}/{SCHEMA}/data/raw_audio/LJSpeech\"\n",
    "with tarfile.open(target_file_path, 'r:bz2') as tar_ref:\n",
    "    tar_ref.extractall(extract_to_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create reference dataframe\n",
    "\n",
    "We create a reference dataframe that contains the file paths of the raw audio files. We will use this dataframe to parallelize the inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488a7973-90e8-41ad-a877-21b8369b0de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_file_reference = spark.createDataFrame(dbutils.fs.ls(f\"/Volumes/{CATALOG}/{SCHEMA}/data/raw_audio/LJSpeech/LJSpeech-1.1/wavs/\"))\\\n",
    "  .withColumn(\"file_path\", F.expr(\"substring(path, 6, length(path))\")) # remove the leading dbfs:/ from the path\n",
    "\n",
    "df_file_reference.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the dataframe to a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5ddecc-2dcc-4a6d-8812-18122d4a2823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_file_reference.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"{CATALOG}.{SCHEMA}.recording_file_reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download models from Hugging Face\n",
    "\n",
    "We download two models from Hugging Face. We do this because it's more efficient to download these larger models once and retrieve them from storage for every batch of inference:\n",
    "- [Whisper-medium](https://huggingface.co/openai/whisper-medium)\n",
    "- [Phi-4](https://huggingface.co/microsoft/phi-4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper-medium is a state-of-the-art automatic speech recognition (ASR) model developed by OpenAI. It is designed to transcribe spoken language into written text with high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHISPER_MODEL_SAVE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/data/models/whisper-medium/\"\n",
    "\n",
    "dbutils.fs.mkdirs(WHISPER_MODEL_SAVE_PATH)\n",
    "\n",
    "whisper_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-medium\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "whisper_pipeline.save_pretrained(WHISPER_MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phi-4 is a state-of-the-art language model developed by Microsoft. It is designed for text generation and can be used for various natural language processing tasks. We will use it for simple classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI_MODEL_SAVE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/data/models/phi-4/\"\n",
    "\n",
    "dbutils.fs.mkdirs(PHI_MODEL_SAVE_PATH)\n",
    "\n",
    "phi_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"microsoft/phi-4\",\n",
    "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "phi_pipeline.save_pretrained(PHI_MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference\n",
    "\n",
    "We run inference on each audio recording and save the results to a Delta table. Our multi-step inference process will:\n",
    "- Convert audio files to a format suitable for model inference using `ConverttoPrompt`.\n",
    "- Transcribe audio recordings into text using the Whisper model with `WhisperTranscription`.\n",
    "- Redact named entities from the transcriptions using `NERRedaction`.\n",
    "- Classify the redacted text into predefined categories using `TextClassification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "from ray.util.spark import setup_ray_cluster\n",
    "\n",
    "import ssl\n",
    "import time\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.assets.audio import AudioAsset\n",
    "import librosa\n",
    "import pydub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Ray on Databricks\n",
    "\n",
    "To set up Ray on Databricks, we need to configure the Ray cluster and allocate resources such as CPU and GPU cores. This setup allows us to leverage Ray's distributed computing capabilities for efficient and scalable inference.\n",
    "\n",
    "For more details, refer to the [Ray on Databricks documentation](https://docs.ray.io/en/latest/ray-on-databricks.html) and [What is Ray on Databricks?](https://docs.databricks.com/aws/en/machine-learning/ray) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpu_cores_per_worker = 20 # number of cores to allocate to Ray per worker\n",
    "num_cpus_head_node = 10 # number of cores to allocate to Ray on the head node\n",
    "num_gpu_per_worker = 1 # number of GPUs to allocate to Ray per worker\n",
    "num_gpus_head_node = 1 # number of GPUs to allocate to Ray on the head node\n",
    "min_worker_nodes = 2 # autoscaling minimum number of workers\n",
    "max_worker_nodes = 2 # autoscaling maximum number of workers\n",
    "\n",
    "ray_conf = setup_ray_cluster(\n",
    "  min_worker_nodes=min_worker_nodes,\n",
    "  max_worker_nodes=max_worker_nodes,\n",
    "  num_cpus_head_node= num_cpus_head_node,\n",
    "  num_gpus_head_node= num_gpus_head_node,\n",
    "  num_cpus_per_node=num_cpu_cores_per_worker,\n",
    "  num_gpus_per_node=num_gpu_per_worker\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define classes to handle each inference step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert .wav file to VLLM prompt-compatible normalized numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConverttoPrompt:\n",
    "    \"\"\"\n",
    "    This class handles the conversion of audio files to a format suitable for model inference.\n",
    "    It reads audio files, converts them to numpy arrays, and normalizes the audio data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, audio_filename):\n",
    "        audio = pydub.AudioSegment.from_wav(audio_filename)\n",
    "        samples = np.array(audio.get_array_of_samples())\n",
    "        if audio.channels == 2:\n",
    "            samples = samples.reshape((-1, 2))\n",
    "\n",
    "        array = np.float32(samples) / 2**15\n",
    "        frame_rate = audio.frame_rate\n",
    "        return array, frame_rate\n",
    "\n",
    "    def __call__(self, row) -> dict:\n",
    "        array, frame_rate = self.transform(row[\"file_path\"])\n",
    "        row[\"array\"] = list(array)\n",
    "        row[\"frame_rate\"] = frame_rate\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Whisper inference on the numpy arrays with VLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperTranscription:\n",
    "    \"\"\"\n",
    "    This class handles the transcription of audio files using the Whisper model.\n",
    "    It reads audio data, converts it to the required format, and performs transcription.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the Whisper model.\n",
    "        max_model_len (int, optional): Maximum length of the model. Defaults to 448.\n",
    "        max_num_seqs (int, optional): Maximum number of sequences. Defaults to 400.\n",
    "        kv_cache_dtype (str, optional): Data type for key-value cache. Defaults to \"fp8\".\n",
    "        gpu_memory_utilization (float, optional): GPU memory utilization. Defaults to 0.5.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        max_model_len: int = 448,\n",
    "        max_num_seqs: int = 400,\n",
    "        kv_cache_dtype: str = \"fp8\",\n",
    "        gpu_memory_utilization: float = 0.5,\n",
    "    ):\n",
    "        self.unverified_context = ssl._create_unverified_context()\n",
    "        self.transcription_pipeline = LLM(\n",
    "            model=str(model_path),\n",
    "            max_model_len=max_model_len,\n",
    "            max_num_seqs=max_num_seqs,\n",
    "            kv_cache_dtype=kv_cache_dtype,\n",
    "            gpu_memory_utilization=gpu_memory_utilization,\n",
    "        )\n",
    "\n",
    "    def transform(self, row):\n",
    "        prompts = []\n",
    "        for array, frame_rate in zip(list(row[\"array\"]), list(row[\"frame_rate\"])):\n",
    "            prompts.append(\n",
    "                {\n",
    "                    \"prompt\": \"<|startoftranscript|>\",\n",
    "                    \"multi_modal_data\": {\"audio\": (array, frame_rate)},\n",
    "                }\n",
    "            )\n",
    "        return prompts\n",
    "\n",
    "    def __call__(self, row) -> str:\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0,\n",
    "            top_p=1.0,\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        prompts = self.transform(row)\n",
    "        outputs = self.transcription_pipeline.generate(prompts, sampling_params)\n",
    "\n",
    "        del row[\"array\"]\n",
    "        del row[\"frame_rate\"]\n",
    "\n",
    "        row[\"transcription\"] = [output.outputs[0].text for output in outputs]\n",
    "\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity redaction replaces any mention of organization, person or location name with asterisks.\n",
    "\n",
    "_Note: this could be done in a more sophisticated manner, here we are just masking mentions of named entities for simplicity._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERRedaction:\n",
    "    \"\"\"\n",
    "    This class handles the redaction of named entities (NER) from transcriptions.\n",
    "    It uses a pre-trained NER model to identify and redact sensitive information such as\n",
    "    organization names, personal names, and locations.\n",
    "\n",
    "    Methods:\n",
    "        redact(text, pipeline): Redacts named entities from the given text using the specified pipeline.\n",
    "        __call__(row): Applies the redaction process to the transcription in the given row.\n",
    "\n",
    "    Attributes:\n",
    "        ner_pipeline: Pre-trained NER model pipeline for entity recognition.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.unverified_context = ssl._create_unverified_context()\n",
    "        self.ner_pipeline = pipeline(\"ner\", device=\"cuda:0\")\n",
    "\n",
    "    def redact(self, text, pipeline):\n",
    "        try:\n",
    "            entities = pipeline(text)\n",
    "            for entity in entities:\n",
    "                if entity['entity'] in ['I-ORG', 'I-PER', 'I-LOC']:\n",
    "                    start, end = entity['start'], entity['end']\n",
    "                    text = text[:start] + '*' * (end - start) + text[end:]\n",
    "\n",
    "            redacted_text = {'redacted_text': text}\n",
    "        except Exception as e:\n",
    "            redacted_text = {'redacted_text': None}\n",
    "        finally:\n",
    "            return redacted_text\n",
    "\n",
    "    def __call__(self, row: dict) -> dict:\n",
    "        text = row[\"transcription\"]\n",
    "        redacted_text = self.redact(text, self.ner_pipeline)\n",
    "\n",
    "        row[\"redacted_text\"] = redacted_text[\"redacted_text\"]\n",
    "\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a class for zero-shot text classification of the transcribed and redacted audio using a general LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassification:\n",
    "    \"\"\"\n",
    "    This class handles the classification of text into predefined categories.\n",
    "    It uses a pre-trained language model to classify the text based on the content.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the classification model.\n",
    "        enforce_eager (bool, optional): Whether to enforce eager execution. Defaults to True.\n",
    "        gpu_memory_utilization (float, optional): GPU memory utilization. Defaults to 0.5.\n",
    "        temperature (float, optional): Sampling temperature for the model. Defaults to 0.5.\n",
    "        max_tokens (int, optional): Maximum number of tokens for the model output. Defaults to 128.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path: str, enforce_eager: bool = True, gpu_memory_utilization: float = 0.5, temperature: float = 0.5, max_tokens: int = 128):\n",
    "        self.unverified_context = ssl._create_unverified_context()\n",
    "        self.cls_pipeline = LLM(\n",
    "            model=model_path,\n",
    "            enforce_eager=enforce_eager,\n",
    "            gpu_memory_utilization=gpu_memory_utilization\n",
    "        )\n",
    "        self.sampling_params = SamplingParams(temperature=temperature, max_tokens=max_tokens)\n",
    "\n",
    "    def create_prompt(self, redacted_text) -> dict:\n",
    "        prompt = [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert at determining the underlying category of a short text passage. Your input is a short text passage and your output is a category. Do not output anything else but one of the following categories that best fits the text passage: 'Politics', 'Sports', 'Entertainment', 'Technology', 'Personal', 'Other'.\"},\n",
    "                {\"role\": \"user\", \"content\": text},\n",
    "            ] for text in redacted_text\n",
    "        ]\n",
    "        return prompt\n",
    "\n",
    "    def __call__(self, row: dict) -> dict:\n",
    "        conversation = self.create_prompt(row[\"redacted_text\"])\n",
    "        outputs = self.cls_pipeline.chat(\n",
    "            conversation,\n",
    "            sampling_params=self.sampling_params,\n",
    "            use_tqdm=False\n",
    "        )\n",
    "        row['classification'] = [output.outputs[0].text for output in outputs]\n",
    "        return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference via Ray Data native commands\n",
    "\n",
    "To run the inference, we will use Ray Data's native commands to parallelize and distribute the workload across multiple nodes. This approach ensures efficient processing of large datasets by leveraging Ray's distributed computing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and create a temporary directory for Ray to use when writing Delta tables\n",
    "temp_dir = f\"/Volumes/{CATALOG}/{SCHEMA}/data/tmp\"\n",
    "dbutils.fs.mkdirs(temp_dir)\n",
    "os.environ[\"RAY_UC_VOLUMES_FUSE_TEMP_DIR\"] = temp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read file reference dataframe from Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_reference = spark.table(f\"{CATALOG}.{SCHEMA}.recording_file_reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray-native Inference Pipeline\n",
    "\n",
    "We run the inference pipeline using Ray Data's native commands to parallelize and distribute the workload across multiple (GPU) nodes.\n",
    "\n",
    "\n",
    "1. **map and map_batches**:\n",
    "   - `map`: This method applies a function to each element of the dataset individually. In this case, `ConverttoPrompt` and `NERRedaction` are applied to each element.\n",
    "   - `map_batches`: This method applies a function to batches of elements in the dataset. Here, `whisper_transcription` and `text_classification` are applied to batches of elements, which can improve performance by reducing the overhead of function calls.\n",
    "\n",
    "2. **num_cpus and num_gpus**:\n",
    "   - `num_cpus`: This argument specifies the number of CPU cores to allocate for each task. For example, `num_cpus=1` allocates one CPU core for each `ConverttoPrompt` task.\n",
    "   - `num_gpus`: This argument specifies the number of GPUs to allocate for each task. For example, `num_gpus=float(40 / 80)` allocates half a GPU for each `whisper_transcription` task.\n",
    "\n",
    "3. **min_size and max_size in ray.data.ActorPoolStrategy**:\n",
    "   - `min_size`: This argument specifies the minimum number of actors (workers) to keep in the pool. For example, `min_size=10` ensures that at least 10 actors are available for `ConverttoPrompt`.\n",
    "   - `max_size`: This argument specifies the maximum number of actors (workers) to keep in the pool. For example, `max_size=100` allows up to 100 actors for `ConverttoPrompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_transcription = WhisperTranscription(model_path=WHISPER_MODEL_SAVE_PATH)\n",
    "text_classification = TextClassification(model_path=PHI_MODEL_SAVE_PATH)\n",
    "\n",
    "ds = ray.data.from_spark(df_file_reference)\n",
    "\n",
    "ds = ds.repartition(200) \\\n",
    "    .map(\n",
    "        ConverttoPrompt,\n",
    "        compute=ray.data.ActorPoolStrategy(min_size=10, max_size=100),\n",
    "        num_cpus=1,\n",
    "    ) \\\n",
    "    .map_batches(\n",
    "        whisper_transcription,\n",
    "        compute=ray.data.ActorPoolStrategy(min_size=3, max_size=6),\n",
    "        num_gpus=float(40 / 80),\n",
    "        batch_size=256\n",
    "    ) \\\n",
    "    .map(\n",
    "        NERRedaction,\n",
    "        compute=ray.data.ActorPoolStrategy(min_size=1, max_size=50),\n",
    "        num_gpus=float(1 / 15)\n",
    "    ) \\\n",
    "    .map_batches(\n",
    "        text_classification,\n",
    "        compute=ray.data.ActorPoolStrategy(min_size=1, max_size=6),\n",
    "        num_gpus=float(40 / 80),\n",
    "        batch_size=256\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the processed audio data to a Delta table in Databricks. This allows us to store the results of our inference pipeline in a structured format that can be easily queried and analyzed. For more details on how to use the `write_databricks_table` method, refer to the [Databricks documentation](https://docs.databricks.com/aws/en/machine-learning/ray/connect-spark-ray#write-ray-data-to-spark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.write_databricks_table(f\"{CATALOG}.{SCHEMA}.processed_audio\", mode='overwrite', mergeSchema=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_download_sample_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
