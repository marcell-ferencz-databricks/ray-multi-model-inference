{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4feb7ad0-fba5-4652-8e79-ff186f5dc467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using >=0.7.0 as it supports whisper and manually updating numba due to conflicts\n",
    "%pip install vllm==0.7.0 pydub numba==0.61.0 databricks-sdk \n",
    "%pip install ray --upgrade\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58074593-f192-4c32-893f-ba128eff74b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Job parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42c44d1b-030d-4c9f-9c6e-60c4176fc636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "params = dbutils.widgets.getAll()\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e3994ee-ed78-497f-a78e-e4a3e81caf94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set catalog, schema and model paths\n",
    "\n",
    "We set the catalog and schema to organise our data and ensure it is stored in the correct location. Change these to suit your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da88cbb2-3bf9-4264-93f1-a6facdc7fff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = params[\"catalog\"]\n",
    "schema = params[\"schema\"]\n",
    "transcription_model_id = params[\"transcription_model_id\"]\n",
    "transcription_model_save_path = f'/Volumes/{catalog}/{schema}/data/models/{transcription_model_id.replace(\"-\", \"_\").replace(\"/\", \"_\")}'\n",
    "llm_model_id = params[\"llm_model_id\"]\n",
    "llm_model_save_path = f'/Volumes/{catalog}/{schema}/data/models/{llm_model_id.replace(\"-\", \"_\").replace(\"/\", \"_\")}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78c8cbe7-f3d8-4c49-b781-22bb68965f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run inference\n",
    "\n",
    "We run inference on each audio recording and save the results to a Delta table. Our multi-step inference process will:\n",
    "- Convert audio files to a format suitable for model inference using `ConverttoPrompt`.\n",
    "- Transcribe audio recordings into text using the Whisper model with `WhisperTranscription`.\n",
    "- Redact named entities from the transcriptions using `NERRedaction`.\n",
    "- Classify the redacted text into predefined categories using `TextClassification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21c67cb8-1ac8-4d78-b843-0eb5b0679f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "from ray.util.spark import setup_ray_cluster\n",
    "\n",
    "import ssl\n",
    "import time\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.assets.audio import AudioAsset\n",
    "from transformers import pipeline\n",
    "import librosa\n",
    "import pydub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4f4498a-fdbf-4237-9b01-5a5f4df00135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set up Ray on Databricks\n",
    "\n",
    "To set up Ray on Databricks, we need to configure the Ray cluster and allocate resources such as CPU and GPU cores. This setup allows us to leverage Ray's distributed computing capabilities for efficient and scalable inference.\n",
    "\n",
    "For more details, refer to the [Ray on Databricks documentation](https://docs.ray.io/en/latest/ray-on-databricks.html) and [What is Ray on Databricks?](https://docs.databricks.com/aws/en/machine-learning/ray) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cb16c82-801d-4744-8ed8-40b52f7835f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_cpu_cores_per_worker = 20 # number of cores to allocate to Ray per worker\n",
    "num_cpus_head_node = 10 # number of cores to allocate to Ray on the head node\n",
    "num_gpu_per_worker = 1 # number of GPUs to allocate to Ray per worker\n",
    "num_gpus_head_node = 1 # number of GPUs to allocate to Ray on the head node\n",
    "min_worker_nodes = 2 # autoscaling minimum number of workers\n",
    "max_worker_nodes = 2 # autoscaling maximum number of workers\n",
    "\n",
    "ray_conf = setup_ray_cluster(\n",
    "  min_worker_nodes=min_worker_nodes,\n",
    "  max_worker_nodes=max_worker_nodes,\n",
    "  num_cpus_head_node= num_cpus_head_node,\n",
    "  num_gpus_head_node= num_gpus_head_node,\n",
    "  num_cpus_per_node=num_cpu_cores_per_worker,\n",
    "  num_gpus_per_node=num_gpu_per_worker\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f52530c-6caa-401e-ab7f-a1a9b3cb9e9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define classes to handle each inference step\n",
    "\n",
    "Ray Data's `map` and `map_batches` expect a callable class, so we define those for each step here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91b253b4-d004-4bc6-b007-4b15b04cf111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Convert .wav file to VLLM prompt-compatible normalized numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f20cf146-2166-4705-9888-58e5a882f217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ConverttoPrompt:\n",
    "    \"\"\"\n",
    "    This class handles the conversion of audio files to a format suitable for model inference.\n",
    "    It reads audio files, converts them to numpy arrays, and normalizes the audio data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, audio_filename):\n",
    "        audio = pydub.AudioSegment.from_wav(audio_filename)\n",
    "        samples = np.array(audio.get_array_of_samples())\n",
    "        if audio.channels == 2:\n",
    "            samples = samples.reshape((-1, 2))\n",
    "\n",
    "        array = np.float32(samples) / 2**15\n",
    "        frame_rate = audio.frame_rate\n",
    "        return array, frame_rate\n",
    "\n",
    "    def __call__(self, row) -> dict:\n",
    "        array, frame_rate = self.transform(row[\"file_path\"])\n",
    "        row[\"array\"] = list(array)\n",
    "        row[\"frame_rate\"] = frame_rate\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f58dc87d-7ee3-46fb-8813-5285853ff50b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run Whisper inference on the numpy arrays with VLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae88a89a-c0fe-44e3-aa79-f5a666260260",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TranscriptionStep:\n",
    "    \"\"\"\n",
    "    This class handles the transcription of audio files using the Whisper model.\n",
    "    It reads audio data, converts it to the required format, and performs transcription.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.unverified_context = ssl._create_unverified_context()\n",
    "        self.transcription_pipeline = LLM(\n",
    "            model=transcription_model_save_path,\n",
    "            max_model_len=448,\n",
    "            max_num_seqs=400,\n",
    "            kv_cache_dtype=\"fp8\",\n",
    "            gpu_memory_utilization=float(40/80),\n",
    "        )\n",
    "\n",
    "    def transform(self, row):\n",
    "        prompts = []\n",
    "        for array, frame_rate in zip(list(row[\"array\"]), list(row[\"frame_rate\"])):\n",
    "            prompts.append(\n",
    "                {\n",
    "                    \"prompt\": \"<|startoftranscript|>\",\n",
    "                    \"multi_modal_data\": {\"audio\": (array, frame_rate)},\n",
    "                }\n",
    "            )\n",
    "        return prompts\n",
    "\n",
    "    def __call__(self, row) -> str:\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0,\n",
    "            top_p=1.0,\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        prompts = self.transform(row)\n",
    "        outputs = self.transcription_pipeline.generate(prompts, sampling_params)\n",
    "\n",
    "        del row[\"array\"]\n",
    "        del row[\"frame_rate\"]\n",
    "\n",
    "        row[\"transcription\"] = [output.outputs[0].text for output in outputs]\n",
    "\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c2718a6-e0c7-4c6d-8998-1edd9da77a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Named Entity redaction replaces any mention of organization, person or location name with asterisks.\n",
    "\n",
    "_Note: this could be done in a more sophisticated manner, here we are just masking mentions of named entities for simplicity._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "802b8493-b684-49e4-bc9a-0bea91b7f8da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RedactionStep:\n",
    "    \"\"\"\n",
    "    This class handles the redaction of named entities (NER) from transcriptions.\n",
    "    It uses a pre-trained NER model to identify and redact sensitive information such as\n",
    "    organization names, personal names, and locations.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.unverified_context = ssl._create_unverified_context()\n",
    "        self.ner_pipeline = pipeline(\"ner\", device=\"cuda:0\")\n",
    "\n",
    "    def redact(self, text, pipeline):\n",
    "        try:\n",
    "            entities = pipeline(text)\n",
    "            for entity in entities:\n",
    "                if entity['entity'] in ['I-ORG', 'I-PER', 'I-LOC']:\n",
    "                    start, end = entity['start'], entity['end']\n",
    "                    text = text[:start] + '*' * (end - start) + text[end:]\n",
    "\n",
    "            redacted_text = {'redacted_text': text}\n",
    "        except Exception as e:\n",
    "            redacted_text = {'redacted_text': None}\n",
    "        finally:\n",
    "            return redacted_text\n",
    "\n",
    "    def __call__(self, row: dict) -> dict:\n",
    "        text = row[\"transcription\"]\n",
    "        redacted_text = self.redact(text, self.ner_pipeline)\n",
    "\n",
    "        row[\"redacted_text\"] = redacted_text[\"redacted_text\"]\n",
    "\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30e7ab3f-3b2f-4d3e-9c09-baac561cf2c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finally, we define a class for zero-shot text classification of the transcribed and redacted audio using a general LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "496188ff-3c6b-43f6-8281-7687f65462b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ClassificationStep:\n",
    "    \"\"\"\n",
    "    This class handles the classification of text into predefined categories.\n",
    "    It uses a pre-trained language model to classify the text based on the content.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.unverified_context = ssl._create_unverified_context()\n",
    "        self.cls_pipeline = LLM(\n",
    "            model=llm_model_save_path,\n",
    "            enforce_eager=True,\n",
    "            gpu_memory_utilization=float(40/80)\n",
    "        )\n",
    "        self.sampling_params = SamplingParams(temperature=0.1, max_tokens=128)\n",
    "\n",
    "    def create_prompt(self, redacted_text) -> dict:\n",
    "        prompt = [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert at determining the underlying category of a short text passage. Your input is a short text passage and your output is a category. Do not output anything else but one of the following categories that best fits the text passage: 'Politics', 'Sports', 'Entertainment', 'Technology', 'Personal', 'Other'.\"},\n",
    "                {\"role\": \"user\", \"content\": text},\n",
    "            ] for text in redacted_text\n",
    "        ]\n",
    "        return prompt\n",
    "\n",
    "    def __call__(self, row: dict) -> dict:\n",
    "        conversation = self.create_prompt(row[\"redacted_text\"])\n",
    "        outputs = self.cls_pipeline.chat(\n",
    "            conversation,\n",
    "            sampling_params=self.sampling_params,\n",
    "            use_tqdm=False\n",
    "        )\n",
    "        row['classification'] = [output.outputs[0].text for output in outputs]\n",
    "        return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a5bae26-2884-41e5-bfef-921ea579e02c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run inference via Ray Data native commands\n",
    "\n",
    "To run the inference, we will use Ray Data's native commands to parallelize and distribute the workload across multiple nodes. This approach ensures efficient processing of large datasets by leveraging Ray's distributed computing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40b836f2-1ba4-4b10-8abc-c1a899759b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define and create a temporary directory for Ray to use when writing Delta tables\n",
    "temp_dir = f\"/Volumes/{catalog}/{schema}/data/tmp\"\n",
    "dbutils.fs.mkdirs(temp_dir)\n",
    "os.environ[\"RAY_UC_VOLUMES_FUSE_TEMP_DIR\"] = temp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6818b1fe-0e63-4854-b033-52e90c50159e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read file reference dataframe from Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "888fce53-a9cf-4a93-97f8-c4742c98a601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_file_reference = spark.table(f\"{catalog}.{schema}.recording_file_reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ec5db6-0b4f-4075-b490-5fb072c04231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We run the inference pipeline using Ray Data's native commands to parallelize and distribute the workload across multiple (GPU) nodes.\n",
    "\n",
    "This is our opportunity to make the pipeline run efficiently.\n",
    "\n",
    "- **map vs. map_batches**:\n",
    "   - Use `map` for operations that are lightweight and need to be applied to each element individually.\n",
    "   - Use `map_batches` for operations that are more computationally intensive and can benefit from batch processing to reduce overhead.\n",
    "\n",
    "- **num_cpus**:\n",
    "   - Allocate more CPU cores for tasks that are CPU-bound and require significant processing power.\n",
    "   - Ensure that the total number of CPU cores allocated does not exceed the available cores in your cluster.\n",
    "\n",
    "- **num_gpus**:\n",
    "   - Allocate GPUs for tasks that can leverage GPU acceleration, such as deep learning inference.\n",
    "   - Ensure that the total number of GPUs allocated does not exceed the available GPUs in your cluster.\n",
    "\n",
    "- **min_size and max_size in ray.data.ActorPoolStrategy**:\n",
    "   - Set `min_size` to ensure a minimum number of actors are always available, which can help maintain a steady throughput.\n",
    "   - Set `max_size` based on the maximum parallelism you want to achieve, considering the available resources and the nature of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af1f4c72-8142-4a61-8d76-dc9846f2fb11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds = ray.data.from_spark(df_file_reference)\n",
    "\n",
    "ds = ds.repartition(200) \\\n",
    "    .map(\n",
    "        ConverttoPrompt,\n",
    "        compute=ray.data.ActorPoolStrategy(min_size=10, max_size=100),\n",
    "        num_cpus=1,\n",
    "    ) \\\n",
    "    .map_batches(\n",
    "        TranscriptionStep,\n",
    "        compute=ray.data.ActorPoolStrategy(min_size=3, max_size=6),\n",
    "        num_gpus=float(40 / 80),\n",
    "        batch_size=256\n",
    "    ) \\\n",
    "    .map(\n",
    "        RedactionStep,\n",
    "        compute=ray.data.ActorPoolStrategy(min_size=1, max_size=45),\n",
    "        num_gpus=float(1 / 15)\n",
    "    ) \\\n",
    "    .map_batches(\n",
    "        ClassificationStep,\n",
    "        compute=ray.data.ActorPoolStrategy(min_size=1, max_size=6),\n",
    "        num_gpus=float(40 / 80),\n",
    "        batch_size=256\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9905878-36ce-4c02-81fd-d13ae88693f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We save the processed audio data to a Delta table in Databricks. This allows us to store the results of our inference pipeline in a structured format that can be easily queried and analyzed. For more details on how to use the `write_databricks_table` method, refer to the [Databricks documentation](https://docs.databricks.com/aws/en/machine-learning/ray/connect-spark-ray#write-ray-data-to-spark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bd11726-c8a3-4d34-9949-ec0062364110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds.write_databricks_table(f\"{catalog}.{schema}.processed_audio\", mode='overwrite', mergeSchema=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_run_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
