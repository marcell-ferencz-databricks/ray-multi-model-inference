{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-model inference with Ray on Databricks\n",
    "\n",
    "This notebook demonstrates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using >=0.7.0 as it supports whisper and manually updating numba due to conflicts\n",
    "%pip install vllm==0.7.0 pydub numba==0.61.0 databricks-sdk \n",
    "%pip install ray --upgrade\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set catalog and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = \"marcell\"\n",
    "SCHEMA = \"call_centre_processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create catalog, schema and volume if they don't exist, and create directories for compressed, raw audio files and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.data\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{CATALOG}/{SCHEMA}/data/compressed/LJSpeech\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{CATALOG}/{SCHEMA}/data/raw_audio/LJSpeech\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{CATALOG}/{SCHEMA}/data/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download raw audio files\n",
    "\n",
    "We download the [LJSpeech dataset](https://paperswithcode.com/dataset/ljspeech) from the URL and unzip it to the raw audio directory. This is a collection of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. The files are stored in a tar.bz2 archive, so we will first download it and then unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the LJSpeech dataset\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
    "target_file_path = f\"/Volumes/{CATALOG}/{SCHEMA}/data/compressed/LJSpeech/LJSpeech-1.1.tar.bz2\"\n",
    "urllib.request.urlretrieve(url, target_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec1f1ea-1573-48e9-b667-f7b60838054f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unzip the LJSpeech dataset\n",
    "\n",
    "import zipfile\n",
    "\n",
    "extract_to_path = f\"/Volumes/{CATALOG}/{SCHEMA}/data/raw_audio/LJSpeech\"\n",
    "with zipfile.ZipFile(target_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create reference dataframe\n",
    "\n",
    "We create a reference dataframe that contains the file paths of the raw audio files. We will use this dataframe to parallelize the inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488a7973-90e8-41ad-a877-21b8369b0de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_file_reference = spark.createDataFrame(dbutils.fs.ls(\"/Volumes/marcell/call_centre_processing/data/raw_audio/LJSpeech/LJSpeech-1.1/wavs/\"))\\\n",
    "  .withColumn(\"file_path\", F.expr(\"substring(path, 6, length(path))\")) # remove the leading dbfs:/ from the path\n",
    "\n",
    "df_file_reference.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the dataframe to a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5ddecc-2dcc-4a6d-8812-18122d4a2823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_file_reference.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"marcell.call_centre_processing.recording_file_reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download models from Hugging Face\n",
    "\n",
    "We download three models from Hugging Face:\n",
    "- [Whisper-medium](https://huggingface.co/openai/whisper-medium)\n",
    "- [Bert-large-uncased](https://huggingface.co/bert-large-uncased) (Hugging Face's default model for text classification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHISPER_MODEL_SAVE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/data/models/whisper-medium/\"\n",
    "dbutils.fs.mkdirs(WHISPER_MODEL_SAVE_PATH)\n",
    "whisper_pipeline = pipeline(\"automatic-speech-recognition\", \"openai/whisper-medium\", torch_dtype=torch.float16, device=\"cuda:0\")\n",
    "whisper_pipeline.save_pretrained(WHISPER_MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert-large-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_MODEL_SAVE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/data/models/bert-large-uncased/\"\n",
    "dbutils.fs.mkdirs(NER_MODEL_SAVE_PATH)\n",
    "ner_pipeline = pipeline(\"ner\", device=\"cuda:0\")\n",
    "ner_pipeline.save_pretrained(NER_MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phi-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI_MODEL_SAVE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/data/models/phi-4/\"\n",
    "dbutils.fs.mkdirs(PHI_MODEL_SAVE_PATH)\n",
    "phi_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"microsoft/phi-4\",\n",
    "        model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "phi_pipeline.save_pretrained(PHI_MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference\n",
    "\n",
    "We run inference on the models and save the results to a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "import ssl\n",
    "import time\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.assets.audio import AudioAsset\n",
    "import librosa\n",
    "import pydub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Ray on Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cpu_cores_per_worker = 20 # number of cores to allocate to Ray per worker\n",
    "num_cpus_head_node = 10 # number of cores to allocate to Ray on the head node\n",
    "num_gpu_per_worker = 1 # number of GPUs to allocate to Ray per worker\n",
    "num_gpus_head_node = 1 # number of GPUs to allocate to Ray on the head node\n",
    "min_worker_nodes = 2 # autoscaling minimum number of workers\n",
    "max_worker_nodes = 2 # autoscaling maximum number of workers\n",
    "\n",
    "ray_conf = setup_ray_cluster(\n",
    "  min_worker_nodes=min_worker_nodes,\n",
    "  max_worker_nodes=max_worker_nodes,\n",
    "  num_cpus_head_node= num_cpus_head_node,\n",
    "  num_gpus_head_node= num_gpus_head_node,\n",
    "  num_cpus_per_node=num_cpu_cores_per_worker,\n",
    "  num_gpus_per_node=num_gpu_per_worker\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_download_sample_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
