{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-model inference with Ray on Databricks\n",
    "\n",
    "This notebook demonstrates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using >=0.7.0 as it supports whisper and manually updating numba due to conflicts\n",
    "%pip install vllm==0.7.0 pydub numba==0.61.0 databricks-sdk \n",
    "%pip install ray --upgrade\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set catalog and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = \"marcell\"\n",
    "SCHEMA = \"call_centre_processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create catalog, schema and volume if they don't exist, and create directories for compressed, raw audio files and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.data\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{CATALOG}/{SCHEMA}/data/compressed/LJSpeech\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{CATALOG}/{SCHEMA}/data/raw_audio/LJSpeech\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{CATALOG}/{SCHEMA}/data/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download raw audio files\n",
    "\n",
    "We download the [LJSpeech dataset](https://paperswithcode.com/dataset/ljspeech) from the URL and unzip it to the raw audio directory. This is a collection of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. The files are stored in a tar.bz2 archive, so we will first download it and then unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the LJSpeech dataset\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
    "target_file_path = f\"/Volumes/{CATALOG}/{SCHEMA}/data/compressed/LJSpeech/LJSpeech-1.1.tar.bz2\"\n",
    "urllib.request.urlretrieve(url, target_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec1f1ea-1573-48e9-b667-f7b60838054f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unzip the LJSpeech dataset\n",
    "\n",
    "import zipfile\n",
    "\n",
    "extract_to_path = f\"/Volumes/{CATALOG}/{SCHEMA}/data/raw_audio/LJSpeech\"\n",
    "with zipfile.ZipFile(target_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create reference dataframe\n",
    "\n",
    "We create a reference dataframe that contains the file paths of the raw audio files. We will use this dataframe to parallelize the inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488a7973-90e8-41ad-a877-21b8369b0de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_file_reference = spark.createDataFrame(dbutils.fs.ls(\"/Volumes/marcell/call_centre_processing/data/raw_audio/LJSpeech/LJSpeech-1.1/wavs/\"))\\\n",
    "  .withColumn(\"file_path\", F.expr(\"substring(path, 6, length(path))\")) # remove the leading dbfs:/ from the path\n",
    "\n",
    "df_file_reference.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the dataframe to a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5ddecc-2dcc-4a6d-8812-18122d4a2823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_file_reference.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"marcell.call_centre_processing.recording_file_reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download models from Hugging Face\n",
    "\n",
    "We download two models from Hugging Face:\n",
    "- [Whisper-medium](https://huggingface.co/openai/whisper-medium)\n",
    "- [Phi-4](https://huggingface.co/microsoft/phi-4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHISPER_MODEL_SAVE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/data/models/whisper-medium/\"\n",
    "dbutils.fs.mkdirs(WHISPER_MODEL_SAVE_PATH)\n",
    "whisper_pipeline = pipeline(\"automatic-speech-recognition\", \"openai/whisper-medium\", torch_dtype=torch.float16, device=\"cuda:0\")\n",
    "whisper_pipeline.save_pretrained(WHISPER_MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phi-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI_MODEL_SAVE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/data/models/phi-4/\"\n",
    "dbutils.fs.mkdirs(PHI_MODEL_SAVE_PATH)\n",
    "phi_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"microsoft/phi-4\",\n",
    "        model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "phi_pipeline.save_pretrained(PHI_MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference\n",
    "\n",
    "We run inference on the models and save the results to a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "import ssl\n",
    "import time\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.assets.audio import AudioAsset\n",
    "import librosa\n",
    "import pydub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Ray on Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cpu_cores_per_worker = 20 # number of cores to allocate to Ray per worker\n",
    "num_cpus_head_node = 10 # number of cores to allocate to Ray on the head node\n",
    "num_gpu_per_worker = 1 # number of GPUs to allocate to Ray per worker\n",
    "num_gpus_head_node = 1 # number of GPUs to allocate to Ray on the head node\n",
    "min_worker_nodes = 2 # autoscaling minimum number of workers\n",
    "max_worker_nodes = 2 # autoscaling maximum number of workers\n",
    "\n",
    "ray_conf = setup_ray_cluster(\n",
    "  min_worker_nodes=min_worker_nodes,\n",
    "  max_worker_nodes=max_worker_nodes,\n",
    "  num_cpus_head_node= num_cpus_head_node,\n",
    "  num_gpus_head_node= num_gpus_head_node,\n",
    "  num_cpus_per_node=num_cpu_cores_per_worker,\n",
    "  num_gpus_per_node=num_gpu_per_worker\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define classes to handle each inference step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert .wav file to prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConverttoPrompt:\n",
    "    \"\"\"\n",
    "    This class handles the conversion of audio files to a format suitable for model inference.\n",
    "    It reads audio files, converts them to numpy arrays, and normalizes the audio data.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def transform(self, audio_filenames):\n",
    "        a = pydub.AudioSegment.from_wav(audio_filenames)\n",
    "        y = np.array(a.get_array_of_samples())\n",
    "        if a.channels == 2:\n",
    "            y = y.reshape((-1, 2))\n",
    "\n",
    "        array = np.float32(y) / 2**15\n",
    "        frame_rate =  a.frame_rate\n",
    "        return array,frame_rate\n",
    "\n",
    "\n",
    "    def __call__(self, row) -> str:\n",
    "        array ,frame_rate = self.transform(row[\"file_path\"])\n",
    "        \n",
    "        row['array'] = list(array)\n",
    "        row['frame_rate'] = frame_rate\n",
    "        return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperTranscription:\n",
    "    \"\"\"\n",
    "    This class handles the transcription of audio files using the Whisper model.\n",
    "    It reads audio data, converts it to the required format, and performs transcription.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the Whisper model.\n",
    "        max_model_len (int, optional): Maximum length of the model. Defaults to 448.\n",
    "        max_num_seqs (int, optional): Maximum number of sequences. Defaults to 400.\n",
    "        kv_cache_dtype (str, optional): Data type for key-value cache. Defaults to \"fp8\".\n",
    "        gpu_memory_utilization (float, optional): GPU memory utilization. Defaults to 0.5.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        max_model_len: int = 448,\n",
    "        max_num_seqs: int = 400,\n",
    "        kv_cache_dtype: str = \"fp8\",\n",
    "        gpu_memory_utilization: float = 0.5,\n",
    "    ):\n",
    "        self.unverified_context = ssl._create_unverified_context()\n",
    "        self.transcription_pipeline = LLM(\n",
    "            model=str(model_path),\n",
    "            max_model_len=max_model_len,\n",
    "            max_num_seqs=max_num_seqs,\n",
    "            kv_cache_dtype=kv_cache_dtype,\n",
    "            gpu_memory_utilization=gpu_memory_utilization,\n",
    "        )\n",
    "\n",
    "    def transform(self, row):\n",
    "        prompts = []\n",
    "        for array, frame_rate in zip(list(row[\"array\"]), list(row[\"frame_rate\"])):\n",
    "            prompts.append(\n",
    "                {\n",
    "                    \"prompt\": \"<|startoftranscript|>\",\n",
    "                    \"multi_modal_data\": {\"audio\": (array, frame_rate)},\n",
    "                }\n",
    "            )\n",
    "        return prompts\n",
    "\n",
    "    def __call__(self, row) -> str:\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0,\n",
    "            top_p=1.0,\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        prompts = self.transform(row)\n",
    "        outputs = self.transcription_pipeline.generate(prompts, sampling_params)\n",
    "\n",
    "        del row[\"array\"]\n",
    "        del row[\"frame_rate\"]\n",
    "\n",
    "        row[\"transcription\"] = [output.outputs[0].text for output in outputs]\n",
    "\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity redaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERRedaction:\n",
    "    \"\"\"\n",
    "    This class handles the redaction of named entities (NER) from transcriptions.\n",
    "    It uses a pre-trained NER model to identify and redact sensitive information such as\n",
    "    organization names, personal names, and locations.\n",
    "\n",
    "    Methods:\n",
    "        redact(text, pipeline): Redacts named entities from the given text using the specified pipeline.\n",
    "        __call__(row): Applies the redaction process to the transcription in the given row.\n",
    "\n",
    "    Attributes:\n",
    "        ner_pipeline: Pre-trained NER model pipeline for entity recognition.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.unverified_context = ssl._create_unverified_context()\n",
    "        self.ner_pipeline = pipeline(\"ner\", device=\"cuda:0\")\n",
    "\n",
    "    def redact(self, text, pipeline):\n",
    "        try:\n",
    "            entities = pipeline(text)\n",
    "            for entity in entities:\n",
    "                if entity['entity'] in ['I-ORG', 'I-PER', 'I-LOC']:\n",
    "                    start, end = entity['start'], entity['end']\n",
    "                    text = text[:start] + '*' * (end - start) + text[end:]\n",
    "\n",
    "            redacted_text = {'redacted_text': text}\n",
    "        except Exception as e:\n",
    "            redacted_text = {'redacted_text': None}\n",
    "        finally:\n",
    "            return redacted_text\n",
    "\n",
    "    def __call__(self, row: dict) -> dict:\n",
    "        text = row[\"transcription\"]\n",
    "        redacted_text = self.redact(text, self.ner_pipeline)\n",
    "\n",
    "        row[\"redacted_text\"] = redacted_text[\"redacted_text\"]\n",
    "\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassification:\n",
    "    \"\"\"\n",
    "    This class handles the classification of text into predefined categories.\n",
    "    It uses a pre-trained language model to classify the text based on the content.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the classification model.\n",
    "        enforce_eager (bool, optional): Whether to enforce eager execution. Defaults to True.\n",
    "        gpu_memory_utilization (float, optional): GPU memory utilization. Defaults to 0.5.\n",
    "        temperature (float, optional): Sampling temperature for the model. Defaults to 0.5.\n",
    "        max_tokens (int, optional): Maximum number of tokens for the model output. Defaults to 128.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path: str, enforce_eager: bool = True, gpu_memory_utilization: float = 0.5, temperature: float = 0.5, max_tokens: int = 128):\n",
    "        self.unverified_context = ssl._create_unverified_context()\n",
    "        self.cls_pipeline = LLM(\n",
    "            model=model_path,\n",
    "            enforce_eager=enforce_eager,\n",
    "            gpu_memory_utilization=gpu_memory_utilization\n",
    "        )\n",
    "        self.sampling_params = SamplingParams(temperature=temperature, max_tokens=max_tokens)\n",
    "\n",
    "    def create_prompt(self, redacted_text) -> dict:\n",
    "        prompt = [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert at determining the underlying category of a short text passage. Your input is a short text passage and your output is a category. Do not output anything else but one of the following categories that best fits the text passage: 'Politics', 'Sports', 'Entertainment', 'Technology', 'Personal', 'Other'.\"},\n",
    "                {\"role\": \"user\", \"content\": text},\n",
    "            ] for text in redacted_text\n",
    "        ]\n",
    "        return prompt\n",
    "\n",
    "    def __call__(self, row: dict) -> dict:\n",
    "        conversation = self.create_prompt(row[\"redacted_text\"])\n",
    "        outputs = self.cls_pipeline.chat(\n",
    "            conversation,\n",
    "            sampling_params=self.sampling_params,\n",
    "            use_tqdm=False\n",
    "        )\n",
    "        row['classification'] = [output.outputs[0].text for output in outputs]\n",
    "        return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference via Ray Data native commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = f\"/Volumes/{CATALOG}/{SCHEMA}/data/tmp\"\n",
    "dbutils.fs.mkdirs(temp_dir)\n",
    "os.environ[\"RAY_UC_VOLUMES_FUSE_TEMP_DIR\"] = temp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_reference = spark.table(f\"{CATALOG}.{SCHEMA}.recording_file_reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_transcription = WhisperTranscription(model_path=WHISPER_MODEL_SAVE_PATH)\n",
    "text_classification = TextClassification(model_path=PHI_MODEL_SAVE_PATH)\n",
    "\n",
    "ds = ray.data.from_spark(df_file_reference)\n",
    "\n",
    "ds = ds.repartition(200) \\\n",
    "    .map(\n",
    "        ConverttoPrompt,\n",
    "        compute=ray.data.ActorPoolStrategy(min_size=10, max_size=100),\n",
    "        num_cpus=1,\n",
    "    ) \\\n",
    "    .map_batches(\n",
    "        whisper_transcription,\n",
    "        compute=ray.data.ActorPoolStrategy(min_size=3, max_size=6),\n",
    "        num_gpus=float(40 / 80),\n",
    "        batch_size=256\n",
    "    ) \\\n",
    "    .map(\n",
    "        NERRedaction,\n",
    "        compute=ray.data.ActorPoolStrategy(min_size=1, max_size=50),\n",
    "        num_gpus=float(1 / 15)\n",
    "    ) \\\n",
    "    .map_batches(\n",
    "        text_classification,\n",
    "        compute=ray.data.ActorPoolStrategy(min_size=1, max_size=6),\n",
    "        num_gpus=float(40 / 80),\n",
    "        batch_size=256\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.write_databricks_table(f\"{CATALOG}.{SCHEMA}.processed_audio\", mode='overwrite', mergeSchema=True)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_download_sample_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
