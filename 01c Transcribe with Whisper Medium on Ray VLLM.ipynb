{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf963c0-cce5-47dc-99f6-81093190ee98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using >=0.7.0 as it supports whisper and manually updating numba due to conflicts\n",
    "%pip install vllm==0.7.0 pydub numba==0.61.0 databricks-sdk \n",
    "%pip install ray --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6da52d7-c54e-4594-b0e4-a9afdeb4bd7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e783bbc5-ff99-49c8-8fa5-5b682cf5056f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "import ssl\n",
    "import time\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.assets.audio import AudioAsset\n",
    "import librosa\n",
    "import pydub\n",
    "import numpy as np\n",
    "from mlflow.utils.databricks_utils import get_databricks_env_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ce314d-5a8b-4513-a963-575d391dd62c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "num_cpu_cores_per_worker = 20 # total cpu's present in each node\n",
    "num_cpus_head_node = \t10\n",
    "num_gpu_per_worker = 1\n",
    "num_gpus_head_node = 1\n",
    "\n",
    "\n",
    "# Set databricks credentials as env vars\n",
    "mlflow_dbrx_creds = get_databricks_env_vars(\"databricks\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = mlflow_dbrx_creds['DATABRICKS_HOST']\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = mlflow_dbrx_creds['DATABRICKS_TOKEN']\n",
    "\n",
    "ray_conf = setup_ray_cluster(\n",
    "  min_worker_nodes=2,\n",
    "  max_worker_nodes=2,\n",
    "  num_cpus_head_node= num_cpus_head_node,\n",
    "  num_gpus_head_node= num_gpus_head_node,\n",
    "  num_cpus_per_node=num_cpu_cores_per_worker,\n",
    "  num_gpus_per_node=num_gpu_per_worker\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc151ce3-38c7-42a5-824b-9439ac53e1b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from util import stage_registered_model , flatten_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181f7dac-9b6d-4481-9315-c913246cf85c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Why using vllm  and what was the alternative and why we used it ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df31c6d6-fc81-4840-811f-6878d7805c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ConverttoPrompt:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, audio_filenames):\n",
    "        a = pydub.AudioSegment.from_wav(audio_filenames)\n",
    "        y = np.array(a.get_array_of_samples())\n",
    "        if a.channels == 2:\n",
    "            y = y.reshape((-1, 2))\n",
    "\n",
    "        array = np.float32(y) / 2**15\n",
    "        frame_rate =  a.frame_rate\n",
    "        return array,frame_rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, row) -> str:\n",
    "        array ,frame_rate = self.transform(row[\"file_path\"])\n",
    "        \n",
    "        row['array'] = list(array)\n",
    "        row['frame_rate'] = frame_rate\n",
    "        return row\n",
    "\n",
    "\n",
    "class WhisperTranscription:\n",
    "    def __init__(self):\n",
    "        self.unverified_context = ssl._create_unverified_context()\n",
    "        print(\"Trying to load model...\")\n",
    "        # WHISPER_MODEL_PATH = stage_registered_model(\n",
    "        #                     catalog = 'mlops_pj', \n",
    "        #                     schema = \"gsk_gsc_cfu_count\", \n",
    "        #                     model_name = \"whisper_large_v3\",\n",
    "        #                     version= 2,\n",
    "        #                     local_base_path = \"/local_disk0/models/\",\n",
    "        #                     overwrite = False)\n",
    "        # flatten_folder(WHISPER_MODEL_PATH)\n",
    "        WHISPER_MODEL_PATH = \"/Volumes/marcell/call_centre_processing/data/model_artifacts/whisper-medium/\"\n",
    "        self.transcription_pipeline = LLM(\n",
    "                            model=str(WHISPER_MODEL_PATH),\n",
    "                            max_model_len=448,\n",
    "                            max_num_seqs=400,\n",
    "                            kv_cache_dtype=\"fp8\",\n",
    "                            gpu_memory_utilization = 40/80)\n",
    "        print(\"Model loaded...\")\n",
    "\n",
    "    def transform(self, row):\n",
    "        prompts = []\n",
    "        for array,frame_rate in zip(list(row['array']),list(row['frame_rate'])):\n",
    "            prompts.append({\"prompt\": \"<|startoftranscript|>\",\n",
    "                                \"multi_modal_data\":{\"audio\": (array,frame_rate)}})\n",
    "            \n",
    "        return prompts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, row) -> str:\n",
    "\n",
    "        # Create a sampling params object.\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0,\n",
    "            top_p=1.0,\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        prompts = self.transform(row)\n",
    "\n",
    "        outputs = self.transcription_pipeline.generate(prompts, sampling_params)\n",
    "\n",
    "        del row['array']\n",
    "        del row['frame_rate']\n",
    "\n",
    "        row['transcription'] = [ output.outputs[0].text for output in outputs]\n",
    "\n",
    "        return row\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NERRedaction:\n",
    "    def __init__(self):\n",
    "        self.unverified_context = ssl._create_unverified_context()\n",
    "        print(\"Trying to load NER model...\")\n",
    "        self.ner_pipeline = pipeline(\"ner\", device=\"cuda:0\")\n",
    "        print(\"NER model loaded.\")\n",
    "\n",
    "\n",
    "    def redact(self, text, pipeline):\n",
    "        try:\n",
    "            entities = pipeline(text)\n",
    "            for entity in entities:\n",
    "                if entity['entity'] in ['I-ORG', 'I-PER', 'I-LOC']:\n",
    "                    start, end = entity['start'], entity['end']\n",
    "                    text = text[:start] + '*'*(end - start) + text[end:]\n",
    "\n",
    "            redacted_text = {'redacted_text': text}\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            redacted_text = {'redacted_text': None}\n",
    "        finally:\n",
    "            return redacted_text\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, row:dict) -> dict:\n",
    "        text = row[\"transcription\"]\n",
    "        redacted_text = self.redact(text, self.ner_pipeline)\n",
    "\n",
    "        row[\"redacted_text\"] = redacted_text[\"redacted_text\"]\n",
    "\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "024d8a59-4320-434c-b50b-a8d00068139e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TextClassification:\n",
    "    def __init__(self):\n",
    "        self.unverified_context = ssl._create_unverified_context()\n",
    "        print(\"Trying to load NER model...\")\n",
    "        MODEL_SAVE_PATH = \"/Volumes/marcell/call_centre_processing/data/model_artifacts/phi-4/\"\n",
    "        self.cls_pipeline = LLM(model=MODEL_SAVE_PATH,\n",
    "                                enforce_eager=True,\n",
    "                               gpu_memory_utilization = 40/80)\n",
    "        self.sampling_params = SamplingParams(temperature=0.5,max_tokens=128)\n",
    "        print(\"Classification model loaded.\")\n",
    "\n",
    "    def create_prompt(self,redacted_text) -> dict:\n",
    "\n",
    "        prompt = [[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert at determining the underlying category of a short text passage. Your input is a short text passage and your output is a category. Do not output anything else but one of the following categories that best fits the text passage: 'Politics', 'Sports', 'Entertainment', 'Technology', 'Personal', 'Other'.\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ] for text in redacted_text]\n",
    "\n",
    "        return prompt\n",
    "  \n",
    "    def __call__(self, row:dict) -> dict:\n",
    "        conversation = self.create_prompt(row[\"redacted_text\"])\n",
    "        outputs = self.cls_pipeline.chat(conversation,\n",
    "                   sampling_params=self.sampling_params,\n",
    "                   use_tqdm=False)\n",
    "        row['classification'] = [ output.outputs[0].text for output in outputs]\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a5686f-7159-4984-b1a6-5ba466cefa35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_file_reference = spark.table(\"marcell.call_centre_processing.recording_file_reference\")\n",
    "# df_file_reference = spark.table(\"marcell.call_centre_processing.sample_data\")\n",
    "df_file_reference.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa665a4-923c-4c21-8ba4-745f691335c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Option1 : Code to run the function inside a udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cce29e1-3676-4239-a94c-5d9db89ebb56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@F.pandas_udf(T.StringType())\n",
    "def transcribe_udf(filepaths: pd.Series) -> pd.Series:\n",
    "    import ray\n",
    "    import ray.data\n",
    "\n",
    "    ray.init(ray_conf[1], ignore_reinit_error=True)\n",
    "\n",
    "    @ray.remote\n",
    "    def ray_data_task(ds = None):\n",
    "        ds = ray.data.from_pandas(pd.DataFrame(filepaths.to_list(),columns = ['file_path']))\n",
    "        print(\"Length of filepaths \", len(filepaths))\n",
    "        preds = (\n",
    "        ds.repartition(filepaths.shape[0])\n",
    "        .map(\n",
    "            ConverttoPrompt,\n",
    "            compute=ray.data.ActorPoolStrategy(min_size=1,max_size=40),\n",
    "            num_cpus=1,\n",
    "        )\n",
    "        .map_batches(\n",
    "              WhisperTranscription,\n",
    "              compute=ray.data.ActorPoolStrategy(min_size=1,max_size=40),\n",
    "              num_gpus=float(40/80),\n",
    "              batch_size = 256\n",
    "          )\n",
    "        )\n",
    "\n",
    "        final_df = preds.to_pandas()\n",
    "\n",
    "        return final_df[\"transcription\"]\n",
    "    \n",
    "    return ray.get(ray_data_task.remote(filepaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eba4646e-d5c5-4fc0-a68d-d63d045d10b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_transcriptions = df_file_reference.repartition(1).withColumn(\"transcription_medium\", transcribe_udf(F.col(\"file_path\")))\n",
    "# df_transcriptions.write.mode(\"overwrite\").saveAsTable(\"marcell.call_centre_processing.whisper_medium_transcriptions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e11e2a4-5913-400d-acb7-4d47599c3a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Option 2 : Run it via Ray Data native commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1a78a5-bb4c-42a0-a44a-304675975c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RAY_UC_VOLUMES_FUSE_TEMP_DIR\"] = \"/Volumes/marcell/call_centre_processing/data/tempDoc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2007c802-27fe-49d2-afae-ceebdbeacac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_file_reference.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ffb6f9-82a3-4383-bfbe-e2e2c124572b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds = ray.data.from_spark(df_file_reference)\n",
    "ds =ds.repartition(200)\\\n",
    "        .map(\n",
    "            ConverttoPrompt,\n",
    "            compute=ray.data.ActorPoolStrategy(min_size=10,max_size=100),\n",
    "            num_cpus=1,\n",
    "        )\\\n",
    "        .map_batches(\n",
    "              WhisperTranscription,\n",
    "              compute=ray.data.ActorPoolStrategy(min_size=3,max_size=6),\n",
    "              num_gpus=float(40/80),\n",
    "              batch_size = 256\n",
    "          )\\\n",
    "          .map(\n",
    "              NERRedaction,\n",
    "              compute=ray.data.ActorPoolStrategy(min_size=1,max_size=50),\n",
    "              num_gpus=float(1/15)\n",
    "          )\\\n",
    "         .map_batches(\n",
    "              TextClassification,\n",
    "              compute=ray.data.ActorPoolStrategy(min_size=1,max_size=6),\n",
    "              num_gpus=float(40/80),\n",
    "              batch_size = 256\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2752c004-4308-4f07-9da8-c32139286935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# binary search is the way to think about this.\n",
    "# parallelism and guessing the right number of workers is the way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8050baa4-f18e-4c44-8f99-361472653878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# give the logic about how you would go about running the same workload  in lower tier hardware and then \n",
    "# and give chunky verbose content explanation on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3280dbf-507e-4b07-b44c-05a5f5eff195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# test the logic DE and get them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38764113-1bfa-408f-8aff-c27760193353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ds.write_databricks_table(\"marcell.call_centre_processing.whisper_medium_transcriptions_vllm_2\", mode = 'overwrite', mergeSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3cbcd8e-d80f-4b54-a8a5-07701e065b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"marcell.call_centre_processing.whisper_medium_transcriptions_vllm_2\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01c Transcribe with Whisper Medium on Ray VLLM",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
